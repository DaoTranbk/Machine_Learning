{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>CleanedSummary</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>944092800</td>\n",
       "      <td>entertainingl funny</td>\n",
       "      <td>beetlejuic well written movi everyth excel act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>944438400</td>\n",
       "      <td>modern day fairy tale</td>\n",
       "      <td>twist rumplestiskin captur film star michael k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>948240000</td>\n",
       "      <td>clamshell edition edited version</td>\n",
       "      <td>alway enjoy movi funni entertain didnt hesit p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>951523200</td>\n",
       "      <td>bettlejuice bettlejuice bettlejuice</td>\n",
       "      <td>happen say name three time michael keaten star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>961718400</td>\n",
       "      <td>great product</td>\n",
       "      <td>realli good idea final product outstand use de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score       Time                       CleanedSummary  \\\n",
       "0      1  944092800                  entertainingl funny   \n",
       "1      1  944438400                modern day fairy tale   \n",
       "2      0  948240000     clamshell edition edited version   \n",
       "3      1  951523200  bettlejuice bettlejuice bettlejuice   \n",
       "4      1  961718400                        great product   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  beetlejuic well written movi everyth excel act...  \n",
       "1  twist rumplestiskin captur film star michael k...  \n",
       "2  alway enjoy movi funni entertain didnt hesit p...  \n",
       "3  happen say name three time michael keaten star...  \n",
       "4  realli good idea final product outstand use de...  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"data\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression for BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122110, 42264)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vect = CountVectorizer()\n",
    "bow = bow_vect.fit_transform(data['CleanedText'].values)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=bow[:,:20000]\n",
    "y=data.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_params = [{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### building the model using L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'accuracy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.821281358338\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features\n",
      "aw\n",
      "disgust\n",
      "horribl\n",
      "disappoint\n",
      "cancel\n",
      "ined\n",
      "flavorless\n",
      "bland\n",
      "dissapoint\n",
      "garbag\n",
      "horrid\n",
      "disapoint\n",
      "bewar\n",
      "gross\n",
      "elsewher\n",
      "drinkabl\n",
      "inferior\n",
      "edibl\n",
      "bare\n",
      "decept\n",
      "concept\n",
      "donat\n",
      "descript\n",
      "burnt\n",
      "embarrass\n",
      "broke\n",
      "bad\n",
      "excit\n",
      "hope\n",
      "defect\n",
      "chalki\n",
      "crap\n",
      "fail\n",
      "china\n",
      "dissappoint\n",
      "deceiv\n",
      "idea\n",
      "dissatisfi\n",
      "contact\n",
      "discard\n",
      "funki\n",
      "dull\n",
      "gag\n",
      "broken\n",
      "googl\n",
      "away\n",
      "earth\n",
      "cardboard\n",
      "guess\n",
      "advertis\n"
     ]
    }
   ],
   "source": [
    "idx=(clf.coef_).argsort()[:1,:50]\n",
    "print('Top 50 features')\n",
    "for i in idx[0]:\n",
    "    print(bow_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.5221248601\n",
      "[[13409  3064]\n",
      " [ 3705 16455]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.3821128470403545"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred,normalize=True)*float(100)\n",
    "print(acc)\n",
    "print(confusion_matrix(y_test,y_pred).T)\n",
    "log_loss(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i.e 844 features out of 20000 selected are important features according to l1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### therefore as C decreases the model begins to uderfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10392\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by increasing C the model begins to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression for TFIDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122110, 42264)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vect=TfidfVectorizer()\n",
    "tf_idf=tf_idf_vect.fit_transform(data['CleanedText'].values)\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=bow[:,:20000]\n",
    "y=data.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122110,)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_params = [{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### building the model using L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'accuracy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.820653509131\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features\n",
      "aw\n",
      "horribl\n",
      "disgust\n",
      "ined\n",
      "disappoint\n",
      "cancel\n",
      "flavorless\n",
      "bland\n",
      "bewar\n",
      "dissapoint\n",
      "decept\n",
      "gross\n",
      "inferior\n",
      "garbag\n",
      "donat\n",
      "disapoint\n",
      "horrid\n",
      "defect\n",
      "deceiv\n",
      "dissappoint\n",
      "elsewher\n",
      "descript\n",
      "concept\n",
      "hope\n",
      "bare\n",
      "earth\n",
      "edibl\n",
      "broke\n",
      "bad\n",
      "excit\n",
      "fals\n",
      "contact\n",
      "drinkabl\n",
      "burnt\n",
      "embarrass\n",
      "fda\n",
      "cardboard\n",
      "broken\n",
      "away\n",
      "fail\n",
      "crap\n",
      "idea\n",
      "discard\n",
      "compart\n",
      "guess\n",
      "gritti\n",
      "china\n",
      "gag\n",
      "flat\n",
      "danger\n"
     ]
    }
   ],
   "source": [
    "idx=(clf.coef_).argsort()[:1,:50]\n",
    "print('Top 50 features')\n",
    "for i in idx[0]:\n",
    "    print(tf_idf_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### accuracy confusion matrix and log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.5002866268\n",
      "[[13534  3156]\n",
      " [ 3621 16322]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.3896536721154265"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred,normalize=True)*float(100)\n",
    "print(acc)\n",
    "print(confusion_matrix(y_test,y_pred).T)\n",
    "log_loss(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by decreasing C model begins to uderfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10347\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By increasing C the model begins to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V model - 100D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_w2v_100 = pd.read_pickle(\"avg_w2v_vec_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>-0.000326</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>-0.000584</td>\n",
       "      <td>-0.001169</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000212</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>-0.001271</td>\n",
       "      <td>-0.001240</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>-0.000512</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000780</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>-0.000452</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>-0.000837</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.000778 -0.000347  0.000871 -0.000326  0.000397  0.000361  0.000081   \n",
       "1  0.000567  0.000178  0.000499  0.000169 -0.000260  0.000105 -0.000142   \n",
       "2 -0.000123 -0.000488  0.000094 -0.001271 -0.001240  0.000217  0.001422   \n",
       "3  0.000780 -0.000327  0.000449 -0.000747 -0.000130 -0.000132  0.000667   \n",
       "4  0.000114  0.001019 -0.000837  0.000060 -0.000303 -0.000609 -0.000855   \n",
       "\n",
       "        7         8         9   ...        91        92        93        94   \\\n",
       "0  0.001098  0.000355  0.000008 ...  -0.000193  0.000537  0.000594  0.000397   \n",
       "1  0.000246 -0.000276 -0.000637 ...  -0.000212  0.000260  0.000580  0.000210   \n",
       "2 -0.000463 -0.000512  0.001132 ...   0.000842  0.000572  0.000398  0.000072   \n",
       "3 -0.000025 -0.000042  0.000242 ...  -0.000111  0.000417  0.000003 -0.000190   \n",
       "4  0.000182  0.000044  0.000346 ...  -0.000147 -0.000636  0.000251  0.001050   \n",
       "\n",
       "        95        96        97        98        99   100  \n",
       "0 -0.000584 -0.001169  0.000315 -0.000140  0.000049    1  \n",
       "1  0.000176  0.000737  0.000288  0.000127  0.000436    1  \n",
       "2  0.000744  0.000469 -0.000092 -0.000362 -0.000065    0  \n",
       "3 -0.000268  0.000645  0.000049 -0.000225 -0.000452    1  \n",
       "4  0.000173 -0.000852 -0.000106  0.000595  0.000065    1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_w2v_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122109, 101)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_w2v_100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=avg_w2v_100.iloc[:,:100].values\n",
    "y=avg_w2v_100.iloc[:,100].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X=s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122109,)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_params = [{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building model using L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'accuracy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.707531460705\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding important features using L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features\n",
      "abandon\n",
      "aaaahhhhhh\n",
      "aboard\n",
      "abc\n",
      "aarrgh\n",
      "abpv\n",
      "abid\n",
      "aauc\n",
      "abcess\n",
      "abit\n",
      "abdi\n",
      "abod\n",
      "aamazon\n",
      "abbott\n",
      "abalon\n",
      "aad\n",
      "abrad\n",
      "abottl\n",
      "abnoxi\n",
      "abotu\n",
      "abound\n",
      "abita\n",
      "aasanfood\n",
      "aaa\n",
      "abbrevi\n",
      "aarti\n",
      "abba\n",
      "aap\n",
      "aaah\n",
      "abb\n",
      "abel\n",
      "abouy\n",
      "aborb\n",
      "abouut\n",
      "abbazabba\n",
      "ablaz\n",
      "aaaaaaarrrrrggghhh\n",
      "aadult\n",
      "aback\n",
      "abraham\n",
      "abe\n",
      "abd\n",
      "aaaarrrrghh\n",
      "aachen\n",
      "abnorm\n",
      "abosolut\n",
      "abouit\n",
      "aardvark\n",
      "abov\n",
      "aaaaaah\n"
     ]
    }
   ],
   "source": [
    "idx=(clf.coef_).argsort()[:1,:50]\n",
    "print('Top 50 features')\n",
    "for i in idx[0]:\n",
    "    print(avg_w2v_100.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.7886331996\n",
      "[[11361  4990]\n",
      " [ 5711 14571]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.089373316486858"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred,normalize=True)*float(100)\n",
    "print(acc)\n",
    "print(confusion_matrix(y_test,y_pred).T)\n",
    "log_loss(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 99 out of 100 features are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### as C decreases, the model begins to underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C increases, the model begins to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V 200D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_w2v_200 = pd.read_pickle(\"avg_w2v_vec_200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=avg_w2v_200.iloc[:,:200].values\n",
    "y=avg_w2v_200.iloc[:,200].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X=s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_params = [{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building model using L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'accuracy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.756094231977\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features\n",
      "abandon\n",
      "absens\n",
      "accompli\n",
      "accidentley\n",
      "aboard\n",
      "abdi\n",
      "aarrgh\n",
      "absout\n",
      "absoulut\n",
      "absolutley\n",
      "abroadway\n",
      "abod\n",
      "absolutey\n",
      "acciugh\n",
      "abuelita\n",
      "accod\n",
      "abstract\n",
      "acceptalbl\n",
      "absorb\n",
      "abc\n",
      "absolutelt\n",
      "acceler\n",
      "accent\n",
      "abcess\n",
      "accordng\n",
      "aad\n",
      "acccompani\n",
      "abound\n",
      "acadami\n",
      "absinthett\n",
      "abit\n",
      "abund\n",
      "aamazon\n",
      "abswer\n",
      "abysm\n",
      "abpv\n",
      "abid\n",
      "abottl\n",
      "abalon\n",
      "accompain\n",
      "abbrevi\n",
      "abbott\n",
      "aback\n",
      "abut\n",
      "abnoxi\n",
      "abotu\n",
      "abraham\n",
      "acceptal\n",
      "absolout\n",
      "abb\n"
     ]
    }
   ],
   "source": [
    "idx=(clf.coef_).argsort()[:1,:50]\n",
    "print('Top 50 features')\n",
    "for i in idx[0]:\n",
    "    print(avg_w2v_200.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.6258018726\n",
      "[[12567  4227]\n",
      " [ 4702 15137]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.4186524209667315"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred,normalize=True)*float(100)\n",
    "print(acc)\n",
    "print(confusion_matrix(y_test,y_pred).T)\n",
    "log_loss(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C decreases the model begins to underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C increases the model begins to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG W2V 300D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_w2v_300 = pd.read_pickle(\"avg_w2v_vec_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=avg_w2v_300.iloc[:,:300].values\n",
    "y=avg_w2v_300.iloc[:,300].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X=s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_params = [{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building model using L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'accuracy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.780307373134\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features\n",
      "abandon\n",
      "accompli\n",
      "accidentley\n",
      "absens\n",
      "abroadway\n",
      "absorb\n",
      "absolutey\n",
      "aboard\n",
      "absoulut\n",
      "abdi\n",
      "acceptalbl\n",
      "acceler\n",
      "accual\n",
      "acadami\n",
      "acdept\n",
      "aarrgh\n",
      "abstract\n",
      "acrospir\n",
      "aad\n",
      "achiva\n",
      "acetaminophen\n",
      "absolutelt\n",
      "abod\n",
      "ackward\n",
      "absout\n",
      "aamazon\n",
      "abuelita\n",
      "absinthett\n",
      "acciugh\n",
      "accod\n",
      "accoutr\n",
      "acini\n",
      "acidosi\n",
      "acoust\n",
      "absolutley\n",
      "abcess\n",
      "actii\n",
      "aceton\n",
      "abe\n",
      "abit\n",
      "acccompani\n",
      "acknowledg\n",
      "abbrevi\n",
      "acehardwareoutlet\n",
      "aadult\n",
      "abnoxi\n",
      "abbott\n",
      "aback\n",
      "acount\n",
      "accompain\n"
     ]
    }
   ],
   "source": [
    "idx=(clf.coef_).argsort()[:1,:50]\n",
    "print('Top 50 features')\n",
    "for i in idx[0]:\n",
    "    print(avg_w2v_300.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.9270057052\n",
      "[[12976  3932]\n",
      " [ 4154 15571]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.623832813500746"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred,normalize=True)*float(100)\n",
    "print(acc)\n",
    "print(confusion_matrix(y_test,y_pred).T)\n",
    "log_loss(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C decreases, model begins to underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C increases the model begins to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Weighted W2V 100D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_avg_w2v_100 = pd.read_pickle(\"tfidf_avg_vec_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001005</td>\n",
       "      <td>-0.000993</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.001377</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.001434</td>\n",
       "      <td>-0.000971</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>-0.001934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.001048</td>\n",
       "      <td>-0.000657</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>-0.000835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000874</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000653</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>-0.001726</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000796</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000939</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>-0.000879</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>-0.001296</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000479</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>-0.001107</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.000634</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.000369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>-0.001132</td>\n",
       "      <td>-0.001522</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>-0.000874</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>-0.000921</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.001005 -0.000993 -0.000176 -0.000016 -0.000637  0.001346  0.000667   \n",
       "1  0.000447  0.000047 -0.000051  0.000002  0.000638  0.000663 -0.001048   \n",
       "2  0.002199  0.000842 -0.001726 -0.000084 -0.000796  0.000935  0.000353   \n",
       "3  0.000479 -0.000181  0.000898  0.000729 -0.001107  0.000479 -0.000218   \n",
       "4  0.001807  0.000264 -0.001362 -0.001132 -0.001522  0.000012  0.001637   \n",
       "\n",
       "        7         8         9   ...        91        92        93        94   \\\n",
       "0  0.001510 -0.000133 -0.000779 ...   0.001775 -0.000025 -0.001377  0.000550   \n",
       "1 -0.000657  0.000902 -0.000835 ...  -0.000874 -0.000076 -0.000138  0.000227   \n",
       "2  0.000058  0.001372 -0.000609 ...  -0.000939  0.000222  0.000223 -0.000879   \n",
       "3 -0.000634 -0.000136 -0.000369 ...  -0.000357  0.000199  0.000379  0.001250   \n",
       "4 -0.000874  0.000608  0.000235 ...   0.001261  0.000399 -0.000921  0.001763   \n",
       "\n",
       "        95        96        97        98        99   100  \n",
       "0 -0.000228 -0.001434 -0.000971  0.000378 -0.001934    1  \n",
       "1  0.000570 -0.000014 -0.000653  0.000828 -0.000387    1  \n",
       "2 -0.001259 -0.001296  0.000505 -0.000455  0.001263    0  \n",
       "3 -0.000416  0.000148  0.000612 -0.000012  0.000545    1  \n",
       "4 -0.000563 -0.001746  0.001239  0.001081  0.000124    1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_avg_w2v_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tfidf_avg_w2v_100.iloc[:,:100].values\n",
    "y=tfidf_avg_w2v_100.iloc[:,100].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X=s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_params = [{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model using L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'accuracy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.648868506538\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features\n",
      "aap\n",
      "abou\n",
      "abhor\n",
      "ab\n",
      "abotu\n",
      "abbott\n",
      "abbazabba\n",
      "aahhh\n",
      "abject\n",
      "aaaarrrrghh\n",
      "aasanfood\n",
      "abdomin\n",
      "abernook\n",
      "aaah\n",
      "abid\n",
      "aaaaallll\n",
      "abbay\n",
      "abra\n",
      "abdomen\n",
      "abiet\n",
      "aad\n",
      "aappubl\n",
      "aborb\n",
      "abit\n",
      "aboutif\n",
      "aaaahhhhhh\n",
      "aberr\n",
      "abalon\n",
      "abbot\n",
      "abat\n",
      "aagh\n",
      "aachen\n",
      "abe\n",
      "abdi\n",
      "aaaaaahhhhh\n",
      "abl\n",
      "abc\n",
      "aaaaaaaaagghh\n",
      "abraham\n",
      "abnorm\n",
      "aaaaa\n",
      "aaaallll\n",
      "aamazon\n",
      "abba\n",
      "abbi\n",
      "aaaaaaaaaaaaaa\n",
      "abomin\n",
      "abrad\n",
      "aback\n",
      "abolut\n"
     ]
    }
   ],
   "source": [
    "idx=(clf.coef_).argsort()[:1,:50]\n",
    "print('Top 50 features')\n",
    "for i in idx[0]:\n",
    "    print(tfidf_avg_w2v_100.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.9086888871\n",
      "[[ 9843  5404]\n",
      " [ 7451 13935]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.120272114132865"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred,normalize=True)*float(100)\n",
    "print(acc)\n",
    "print(confusion_matrix(y_test,y_pred).T)\n",
    "log_loss(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C decreases model begins to underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=100, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C increases the model begins to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Weighted W2V 200D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_avg_w2v_200 = pd.read_pickle(\"tfidf_avg_vec_200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tfidf_avg_w2v_200.iloc[:,:200].values\n",
    "y=tfidf_avg_w2v_200.iloc[:,200].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X=s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_params = [{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building model using L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'accuracy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.702399475882\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features\n",
      "accentu\n",
      "academia\n",
      "accent\n",
      "abnoxi\n",
      "absolout\n",
      "aborio\n",
      "abhor\n",
      "aaaaaah\n",
      "abroad\n",
      "aaaaallll\n",
      "aaaallll\n",
      "abour\n",
      "accient\n",
      "abbey\n",
      "aachen\n",
      "abdi\n",
      "abrubt\n",
      "acciugh\n",
      "abrotanum\n",
      "acadami\n",
      "absolut\n",
      "access\n",
      "absolutley\n",
      "abdomen\n",
      "abcstor\n",
      "abit\n",
      "abv\n",
      "abat\n",
      "absentmind\n",
      "accept\n",
      "aargh\n",
      "accordng\n",
      "abpv\n",
      "absurt\n",
      "accidentley\n",
      "acclaim\n",
      "abovi\n",
      "aah\n",
      "aamazon\n",
      "accompain\n",
      "aaaaaaaaagghh\n",
      "abou\n",
      "abil\n",
      "aboard\n",
      "aaah\n",
      "acccompani\n",
      "abita\n",
      "abuelita\n",
      "abbrevi\n",
      "abottl\n"
     ]
    }
   ],
   "source": [
    "idx=(clf.coef_).argsort()[:1,:50]\n",
    "print('Top 50 features')\n",
    "for i in idx[0]:\n",
    "    print(tfidf_avg_w2v_200.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy confusion matrix and log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.2290284716\n",
      "[[11170  5006]\n",
      " [ 5900 14557]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.282658067527477"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred,normalize=True)*float(100)\n",
    "print(acc)\n",
    "print(confusion_matrix(y_test,y_pred).T)\n",
    "log_loss(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C decreases the model begins to underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C increases the model begins to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Weighted W2V 300D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_avg_w2v_300 = pd.read_pickle(\"tfidf_avg_vec_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tfidf_avg_w2v_300.iloc[:,:300].values\n",
    "y=tfidf_avg_w2v_300.iloc[:,300].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X=s.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_params = [{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model using L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'accuracy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.708732563536\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 298 out of 300 are important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 features\n",
      "aaaarrrrghh\n",
      "aaaaaaaaaaaaaa\n",
      "accross\n",
      "accident\n",
      "actii\n",
      "absolutley\n",
      "aaa\n",
      "acerb\n",
      "acclim\n",
      "achiot\n",
      "accomplish\n",
      "acrid\n",
      "acoupl\n",
      "acc\n",
      "abililti\n",
      "aaaaaaarrrrrggghhh\n",
      "aafco\n",
      "acidophilus\n",
      "abscess\n",
      "accuraci\n",
      "acoust\n",
      "acic\n",
      "accostum\n",
      "abus\n",
      "abe\n",
      "acdept\n",
      "accompany\n",
      "account\n",
      "achill\n",
      "abit\n",
      "ace\n",
      "abrad\n",
      "accidentley\n",
      "acai\n",
      "accutec\n",
      "abberlin\n",
      "accompain\n",
      "achiev\n",
      "abstin\n",
      "abosult\n",
      "acl\n",
      "absoluet\n",
      "acetaia\n",
      "achiva\n",
      "absent\n",
      "acetaminophen\n",
      "absurd\n",
      "aboout\n",
      "abnoxi\n",
      "abscond\n"
     ]
    }
   ],
   "source": [
    "idx=(clf.coef_).argsort()[:1,:50]\n",
    "print('Top 50 features')\n",
    "for i in idx[0]:\n",
    "    print(tfidf_avg_w2v_300.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy confusion matrix and log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.8787159119\n",
      "[[11342  4925]\n",
      " [ 5743 14623]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.058260548383938"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred,normalize=True)*float(100)\n",
    "print(acc)\n",
    "print(confusion_matrix(y_test,y_pred).T)\n",
    "log_loss(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0001, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C decreases the model begins to underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, penalty = 'l1')\n",
    "clf.fit(X_train, y_train)\n",
    "w = clf.coef_    ## weight vector\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As C increases, the model begins to overfit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
